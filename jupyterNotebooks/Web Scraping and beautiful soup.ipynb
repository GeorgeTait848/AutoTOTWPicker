{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe2480c",
   "metadata": {},
   "source": [
    "# Web scraping and beatiful soup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb762fa",
   "metadata": {},
   "source": [
    "Web scraping is a process (often automated) of gathering information from websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee9d363",
   "metadata": {},
   "source": [
    "## Initial steps \n",
    "\n",
    "Firstly, youll want to familiarise yourself with the website that you want to scrape. For my automatic TOTW picker, i want to scrape the [HCPCL Website](https://hcpcl.play-cricket.com) and the [Cherwell League Website](https://www.cherwellcricketleague.com) for scorecards of the four Oxford CC Scorecards from any given date. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca44a8",
   "metadata": {},
   "source": [
    "## Deciphering URLs\n",
    "\n",
    "The URL of a webpage will tell you alot of information about the location of that webpage, for example: \n",
    "\n",
    "https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html\n",
    "\n",
    "can be deconstructed into two main parts: \n",
    "\n",
    "1. The base URL which is realpython.github.io/fake-jobs/\n",
    "2. The site location\n",
    "\n",
    "Any job posted on this website will use the same base URL but the site location (ending with .html) will be different. \n",
    "\n",
    "\n",
    "URLs can also have query paramters encoded in them if youre performing a search. These paramaters are sent as query strings to a database to retrieve the necessary records. \n",
    "\n",
    "The beginnning of a query is denoted by a `?`\n",
    "\n",
    "Information is encoded in key-value pairs (key=value), and if multiple pairs are used theyre separated by `&`. \n",
    "\n",
    "E.g: https://au.indeed.com/jobs?q=developer&l=perth will search the indeed database for developer jobs in perth. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6c058",
   "metadata": {},
   "source": [
    "## Inspecting the site with developer tools \n",
    "\n",
    "On mac, use the keystroke CMD + OPTN + I to inspect the HTML of a page, where you can have the webpage and source code as clickable html elements \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778582b3",
   "metadata": {},
   "source": [
    "## Scraping the HTML\n",
    "\n",
    "To get the html from a page, you will need the requests package. Firstly, create a virtual environment and install within in the requests package usings `pip3 install requests` while in the virtual environment. \n",
    "\n",
    "Then, in create a .py file and type the following code: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb3a6fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/prr7tb2x3ls62w0syvfw4v7w0000gp/T/ipykernel_39920/3087354250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://realpython.github.io/fake-jobs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs/\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202dfd8",
   "metadata": {},
   "source": [
    "Note, on this notebook this code will not execute properly as in my jupyter notebook virtual environment i do not have the requests package installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2eebaa",
   "metadata": {},
   "source": [
    "When your python script is executed in terminal, it will print the entire HTML content to terminal, you can redirect this using\n",
    "\n",
    "    python3 exampleWebScraping.py > exampleWebScraping.html\n",
    "\n",
    "This will save all the html from the webpage to a file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb65993",
   "metadata": {},
   "source": [
    "## Parsing HTML code using BeautifulSoup\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
